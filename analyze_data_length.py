#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†é•¿åº¦åˆ†æå·¥å…·
åˆ†æopus100æ•°æ®é›†ä¸­å¥å­çš„é•¿åº¦åˆ†å¸ƒï¼Œå¸®åŠ©ç¡®å®šåˆé€‚çš„åºåˆ—é•¿åº¦
"""

import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from datasets import load_dataset
import numpy as np
from config import get_config

def analyze_sentence_lengths(max_samples=10000):
    """åˆ†ææ•°æ®é›†ä¸­å¥å­çš„é•¿åº¦åˆ†å¸ƒ"""
    print("ğŸ” å¼€å§‹åˆ†æopus100æ•°æ®é›†å¥å­é•¿åº¦åˆ†å¸ƒ...")
    config = get_config()

    try:
        # åŠ è½½æ•°æ®é›†
        print("ğŸ“¥ åŠ è½½opus100 en-zhæ•°æ®é›†...")
        ds_full = load_dataset("opus100", "en-zh")

        # åˆ†æè®­ç»ƒé›†
        train_ds = ds_full['train']
        print(f"è®­ç»ƒé›†æ€»å¤§å°: {len(train_ds)} æ ·æœ¬")

        # å¦‚æœæ•°æ®é›†å¤ªå¤§ï¼Œé‡‡æ ·åˆ†æ
        sample_size = min(max_samples, len(train_ds))
        indices = np.random.choice(len(train_ds), sample_size, replace=False)

        en_lengths = []
        zh_lengths = []
        total_pairs = 0

        print(f"åˆ†æå‰{sample_size}ä¸ªæ ·æœ¬çš„é•¿åº¦åˆ†å¸ƒ...")

        for idx in indices:
            sample = train_ds[int(idx)]
            en_text = sample['translation']['en']
            zh_text = sample['translation']['zh']

            # è‹±æ–‡å­—ç¬¦é•¿åº¦
            en_len = len(en_text)
            # ä¸­æ–‡å­—ç¬¦é•¿åº¦
            zh_len = len(zh_text)

            en_lengths.append(en_len)
            zh_lengths.append(zh_len)
            total_pairs += 1

            if total_pairs % 2000 == 0:
                print(f"  å·²åˆ†æ {total_pairs} ä¸ªæ ·æœ¬...")

        # è½¬æ¢ä¸ºnumpyæ•°ç»„ä¾¿äºåˆ†æ
        en_lengths = np.array(en_lengths)
        zh_lengths = np.array(zh_lengths)

        print(f"\nğŸ“Š æ•°æ®åˆ†æç»“æœ (åŸºäº{total_pairs}ä¸ªæ ·æœ¬):")
        print("="*60)

        # è‹±æ–‡é•¿åº¦ç»Ÿè®¡
        print("ğŸ”¤ è‹±æ–‡å¥å­é•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {np.mean(en_lengths):.1f} å­—ç¬¦")
        print(f"  ä¸­ä½æ•°: {np.median(en_lengths):.1f} å­—ç¬¦")
        print(f"  æ ‡å‡†å·®: {np.std(en_lengths):.1f} å­—ç¬¦")
        print(f"  æœ€å°é•¿åº¦: {np.min(en_lengths)} å­—ç¬¦")
        print(f"  æœ€å¤§é•¿åº¦: {np.max(en_lengths)} å­—ç¬¦")
        print(f"  95%åˆ†ä½æ•°: {np.percentile(en_lengths, 95):.1f} å­—ç¬¦")
        print(f"  99%åˆ†ä½æ•°: {np.percentile(en_lengths, 99):.1f} å­—ç¬¦")

        # ä¸­æ–‡é•¿åº¦ç»Ÿè®¡
        print("\nğŸ‡¨ğŸ‡³ ä¸­æ–‡å¥å­é•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {np.mean(zh_lengths):.1f} å­—ç¬¦")
        print(f"  ä¸­ä½æ•°: {np.median(zh_lengths):.1f} å­—ç¬¦")
        print(f"  æ ‡å‡†å·®: {np.std(zh_lengths):.1f} å­—ç¬¦")
        print(f"  æœ€å°é•¿åº¦: {np.min(zh_lengths)} å­—ç¬¦")
        print(f"  æœ€å¤§é•¿åº¦: {np.max(zh_lengths)} å­—ç¬¦")
        print(f"  95%åˆ†ä½æ•°: {np.percentile(zh_lengths, 95):.1f} å­—ç¬¦")
        print(f"  99%åˆ†ä½æ•°: {np.percentile(zh_lengths, 99):.1f} å­—ç¬¦")

        # è®¡ç®—åˆ†è¯åçš„é•¿åº¦ï¼ˆè€ƒè™‘ç‰¹æ®Šæ ‡è®°ï¼‰
        print(f"\nğŸ§® è€ƒè™‘åˆ°åˆ†è¯åçš„é•¿åº¦é¢„ä¼°:")
        # è‹±æ–‡ï¼šå¹³å‡æ¯ä¸ªå•è¯çº¦5å­—ç¬¦ï¼ŒåŠ ä¸Šç©ºæ ¼
        en_tokenized = en_lengths * 1.2  # ç²—ç•¥ä¼°è®¡
        # ä¸­æ–‡ï¼šå­—ç¬¦çº§åˆ†è¯ï¼ŒåŸºæœ¬ä¿æŒåŸé•¿åº¦ï¼ŒåŠ ä¸Šç‰¹æ®Šæ ‡è®°
        zh_tokenized = zh_lengths * 1.1 + 2  # + SOSå’ŒEOS

        print(f"  è‹±æ–‡åˆ†è¯åå¹³å‡é•¿åº¦: {np.mean(en_tokenized):.1f}")
        print(f"  ä¸­æ–‡åˆ†è¯åå¹³å‡é•¿åº¦: {np.mean(zh_tokenized):.1f}")

        # è®¡ç®—coverç‡
        current_seq_len = config['seq_len']
        en_cover_95 = np.percentile(en_tokenized, 95)
        zh_cover_95 = np.percentile(zh_tokenized, 95)
        max_cover_95 = max(en_cover_95, zh_cover_95)

        print(f"\nğŸ“ ä¸å½“å‰åºåˆ—é•¿åº¦({current_seq_len})çš„å¯¹æ¯”:")
        print(f"  95%çš„è‹±æ–‡å¥å­åˆ†è¯å < {en_cover_95:.1f}")
        print(f"  95%çš„ä¸­æ–‡å¥å­åˆ†è¯å < {zh_cover_95:.1f}")
        print(f"  å»ºè®®åºåˆ—é•¿åº¦(è¦†ç›–95%): {max_cover_95:.0f}")
        print(f"  å»ºè®®åºåˆ—é•¿åº¦(è¦†ç›–99%): {max(np.percentile(en_tokenized, 99), np.percentile(zh_tokenized, 99)):.0f}")

        # è¶…é™ç»Ÿè®¡
        en_over_limit = np.sum(en_tokenized > current_seq_len)
        zh_over_limit = np.sum(zh_tokenized > current_seq_len)

        print(f"\nâš ï¸  è¶…é™æƒ…å†µåˆ†æ:")
        print(f"  è‹±æ–‡è¶…è¿‡{current_seq_len}çš„æ ·æœ¬: {en_over_limit}ä¸ª ({en_over_limit/total_pairs*100:.2f}%)")
        print(f"  ä¸­æ–‡è¶…è¿‡{current_seq_len}çš„æ ·æœ¬: {zh_over_limit}ä¸ª ({zh_over_limit/total_pairs*100:.2f}%)")

        return {
            'en_stats': {
                'mean': np.mean(en_lengths),
                'median': np.median(en_lengths),
                'max': np.max(en_lengths),
                'p95': np.percentile(en_lengths, 95),
                'p99': np.percentile(en_lengths, 99)
            },
            'zh_stats': {
                'mean': np.mean(zh_lengths),
                'median': np.median(zh_lengths),
                'max': np.max(zh_lengths),
                'p95': np.percentile(zh_lengths, 95),
                'p99': np.percentile(zh_lengths, 99)
            },
            'recommendations': {
                'seq_len_95': max_cover_95,
                'seq_len_99': max(np.percentile(en_tokenized, 99), np.percentile(zh_tokenized, 99))
            }
        }

    except Exception as e:
        print(f"âŒ æ•°æ®åˆ†æå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    result = analyze_sentence_lengths(max_samples=5000)  # åˆ†æ5000ä¸ªæ ·æœ¬
    if result:
        print(f"\nâœ… åˆ†æå®Œæˆï¼å»ºè®®çš„åºåˆ—é•¿åº¦: {result['recommendations']['seq_len_95']:.0f}")
    else:
        print("""\nğŸ’¡ ä¸´æ—¶å»ºè®®:
1. å°†åºåˆ—é•¿åº¦è°ƒæ•´ä¸º512æˆ–600ï¼ˆè¦†ç›–95%çš„æ•°æ®ï¼‰
2. æˆ–è€…æ·»åŠ æ•°æ®æˆªæ–­/è¿‡æ»¤é€»è¾‘
3. å¯¹äºç‰¹åˆ«é•¿çš„å¥å­ï¼Œå¯ä»¥è€ƒè™‘æˆªæ–­è€Œä¸æ˜¯å®Œå…¨ä¸¢å¼ƒ""")