#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æˆªæ–­ç»Ÿè®¡å·¥å…·
ç›‘æ§å’ŒæŠ¥å‘Šæ•°æ®æˆªæ–­æƒ…å†µ
"""

import torch
from dataset import CausalLanguageModelingDataset

class TruncationStats:
    """æˆªæ–­ç»Ÿè®¡ç±»ï¼Œç”¨äºç›‘æ§æ•°æ®æˆªæ–­æƒ…å†µ"""

    def __init__(self):
        self.reset()

    def reset(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.total_samples = 0
        self.src_truncated = 0
        self.tgt_truncated = 0

    def update(self, original_src_tokens, truncated_src_tokens, original_tgt_tokens, truncated_tgt_tokens):
        """æ›´æ–°æˆªæ–­ç»Ÿè®¡"""
        self.total_samples += 1
        if len(original_src_tokens) != len(truncated_src_tokens):
            self.src_truncated += 1
        if len(original_tgt_tokens) != len(truncated_tgt_tokens):
            self.tgt_truncated += 1

    def get_stats(self):
        """è·å–ç»Ÿè®¡ç»“æœ"""
        if self.total_samples == 0:
            return {
                'total': 0,
                'src_truncated': 0,
                'tgt_truncated': 0,
                'src_ratio': 0.0,
                'tgt_ratio': 0.0
            }

        return {
            'total': self.total_samples,
            'src_truncated': self.src_truncated,
            'tgt_truncated': self.tgt_truncated,
            'src_ratio': self.src_truncated / self.total_samples * 100,
            'tgt_ratio': self.tgt_truncated / self.total_samples * 100
        }

    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ç»“æœ"""
        stats = self.get_stats()
        print(f"ğŸ“Š æˆªæ–­ç»Ÿè®¡æŠ¥å‘Š:")
        print(f"  æ€»æ ·æœ¬æ•°: {stats['total']}")
        print(f"  æºè¯­è¨€æˆªæ–­æ•°: {stats['src_truncated']} ({stats['src_ratio']:.2f}%)")
        print(f"  ç›®æ ‡è¯­è¨€æˆªæ–­æ•°: {stats['tgt_truncated']} ({stats['tgt_ratio']:.2f}%)")
        print(f"  æ•´ä½“æˆªæ–­ç‡: {(stats['src_ratio'] + stats['tgt_ratio']):.2f}%")